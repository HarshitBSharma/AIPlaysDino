{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Learns Chrome T-Rex Game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lets import these babies\n",
    "\"\"\"\n",
    "\n",
    "# Bread and butter of any DL Problem\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For Automating browser actions\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Importing Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Now this is random\n",
    "import random\n",
    "\n",
    "# Collections\n",
    "from collections import namedtuple\n",
    "\n",
    "# For image processing\n",
    "import cv2\n",
    "\n",
    "# Just because I can't do math\n",
    "import math\n",
    "\n",
    "# Yeah this is the time stone\n",
    "import time\n",
    "\n",
    "import pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"# Setting up display\\n%matplotlib inline\\nis_ipython = 'inline' in matplotlib.get_backend()\\nif is_ipython: from IPython import display\""
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"# Setting up display\n",
    "%matplotlib inline\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Experience(state=1, action=2, next_state=3, reward=5, done=True)"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward', 'done')\n",
    ")\n",
    "\n",
    "e = Experience(1, 2, 3, 5, True)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_episode):\n",
    "        exploration_rate = self.start + (self.end-self.start) *\\\n",
    "                      math.exp(-1. * current_episode * self.decay)\n",
    "\n",
    "        return exploration_rate \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        self.current_episode = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "    \n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_episode)\n",
    "        if rate <= random.random():\n",
    "            # Exploration\n",
    "            return random.randrange(self.num_actions)\n",
    "        else:\n",
    "            # Exploitation\n",
    "            with torch.no_grad():\n",
    "                # print(f\"The agent has decided to take action #{policy_net(state).argmax(dim=1)}\")\n",
    "                q_values = policy_net(state)\n",
    "                action = q_values.argmax(dim=1).item()\n",
    "                return action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is our Enviroment manager \n",
    "chrome_browser_path = \".//Driver/chromedriver.exe\"\n",
    "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
    "game_url = 'chrome://dino'\n",
    "\n",
    "class ChromeManager():\n",
    "    def __init__(self, device):\n",
    "        print(\"Hello\")\n",
    "        self.device = device\n",
    "        self.done = False\n",
    "        self.initialize_chrome()\n",
    "        self.get_state = ImagePreProcessing(self.driver, device)\n",
    "    \n",
    "    def initialize_chrome(self):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"start-maximized\")\n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--mute-audio\")\n",
    "        self.driver = webdriver.Chrome(executable_path=chrome_browser_path, chrome_options=chrome_options)\n",
    "\n",
    "    # Restarting our game\n",
    "    def reset(self):\n",
    "        try:\n",
    "            self.driver.get(game_url)\n",
    "        except:\n",
    "            pass\n",
    "            # print('Environment has been reset')\n",
    "        self.driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self.driver.execute_script(init_script)\n",
    "        return self.driver.execute_script(\"return Runner.instance_.restart()\")\n",
    "\n",
    "    # To close the game\n",
    "    def close(self):\n",
    "        return self.driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "\n",
    "    # crash\n",
    "    def get_crashed(self):\n",
    "        return self.driver.execute_script(\"return Runner.instance_.crashed\")\n",
    "\n",
    "    # Start?\n",
    "    def start(self):\n",
    "        try:\n",
    "            self.driver.get(game_url)\n",
    "        except:\n",
    "            pass\n",
    "            # print('Exception has been handled')\n",
    "        self.driver.execute_script('Runner.instance_.playing=true')\n",
    "        self.press_up()\n",
    "        # time.sleep(3)\n",
    "\n",
    "    # To get the current score of our game\n",
    "    def get_score(self):\n",
    "        score_array = self.driver.execute_script('return Runner.instance_.distanceMeter.digits')\n",
    "        # Scores are stored in the form '0, 1, 4', for a score of 14.\n",
    "        score = ''.join(score_array)\n",
    "        return int(score)\n",
    "\n",
    "    # To get the highscore of our game\n",
    "    def get_highscore(self):\n",
    "        highscore_array = self.driver.execute_script('return Runner.instance_.distanceMeter.highScore')\n",
    "        for i in range(len(highscore_array)):\n",
    "            if highscore_array[i] == 0:\n",
    "                break\n",
    "            highscore_array = highscore_array[i:]\n",
    "            highscore = ''.join(highscore_array)\n",
    "            return int(highscore)\n",
    "    \n",
    "    # This function will be called when we want to jump\n",
    "    def press_up(self):\n",
    "        self.driver.find_element_by_tag_name('body').send_keys(Keys.ARROW_UP)\n",
    "\n",
    "    # This function will be called when we want to duck\n",
    "    def press_down(self):\n",
    "        pyautogui.keyDown(\"down\")\n",
    "        time.sleep(0.2)\n",
    "        pyautogui.keyUp(\"down\")\n",
    "        # self.driver.find_element_by_tag_name('body').send_keys(Keys.ARROW_DOWN)\n",
    "\n",
    "    # This function will be called when we don't want to jump or duck\n",
    "    def press_right(self):\n",
    "        self.driver.find_element_by_tag_name('body').send_keys(Keys.ARROW_RIGHT)\n",
    "\n",
    "    # Taking actions\n",
    "    def take_action(self, action):\n",
    "        score = self.get_score()\n",
    "        highscore = self.get_highscore()\n",
    "        reward = 0.1\n",
    "        if action == 0:\n",
    "            # T-Rex Jumps\n",
    "            self.press_up()\n",
    "        elif action == 1:\n",
    "            # T-Rex ducks\n",
    "            self.press_down()\n",
    "        elif action == 2:\n",
    "            # T-Rex procrastinates\n",
    "            self.press_right()\n",
    "\n",
    "        state = self.get_state.screenshot()\n",
    "        self.done = False\n",
    "        if self.get_crashed():\n",
    "            time.sleep(0.1)\n",
    "            reward = -1\n",
    "            self.done = True\n",
    "        return state, reward, self.done, score, highscore\n",
    "        \n",
    "\n",
    "\n",
    "    # Hard stop\n",
    "    def close_all(self):\n",
    "        self.driver.close()\n",
    "        self.driver.quit()\n",
    "        try:\n",
    "            os.system('cmd /c taskkill /F /IM chromedriver.exe')\n",
    "        except:\n",
    "            print('No tasks found!')\n",
    "\n",
    "\n",
    "class EnviromentManager():\n",
    "    def __init__(self, agent, environment):\n",
    "        self.agent = agent\n",
    "        self.environment = environment\n",
    "\n",
    "\n",
    "class ImagePreProcessing():\n",
    "    def __init__(self, driver, device):\n",
    "        self.driver = driver\n",
    "        self.device = device\n",
    "\n",
    "    # Take Screenshot. We'll be needing 4 screenshots, so yeah\n",
    "    def screenshot(self):\n",
    "        file_path = './/Screenshots/'\n",
    "        current_state = []\n",
    "        for i in range(1, 5):\n",
    "            file_name = file_path + 'Screenshot' + str(i) + '.jpg'\n",
    "            self.driver.save_screenshot(file_name)\n",
    "            image_tensor = self.process_image(file_name)\n",
    "            current_state.append(image_tensor)\n",
    "        current_state = torch.cat(current_state).unsqueeze(0).to(self.device)\n",
    "        print('THis marks the end of th capturing process')\n",
    "        return current_state\n",
    "\n",
    "    \n",
    "    def process_image(self, image_file):\n",
    "        image = cv2.imread(image_file)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        image[image>255] = 255\n",
    "        image = cv2.resize(image, (84, 84))\n",
    "        image = np.reshape(image, (1, 84, 84))\n",
    "        #image = torch.from_numpy(image)\n",
    "        return self.image_to_tensor(image)\n",
    "\n",
    "\n",
    "    def image_to_tensor(self, image):\n",
    "        imag = image.astype(np.float32)\n",
    "        image_tensor = torch.from_numpy(image)\n",
    "        image_tensor = image_tensor.to(self.device, dtype=torch.float32)\n",
    "        return image_tensor\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Starting to Build our DQN\n",
    "class DinoNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DinoNetwork, self).__init__()\n",
    "        self.number_of_actions = 3\n",
    "        self.gamma = 0.99\n",
    "        self.initial_epsilon = 0.1\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.number_of_iterations = 10000\n",
    "        self.replay_memory_size = 1000\n",
    "        self.minibatch_size = 1 \n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 5, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 5, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            # nn.BatchNorm1d(256)\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # print(f\"After last conv layer, shape is {x.shape}\")\n",
    "        x = x.flatten(start_dim=1, end_dim=-1)\n",
    "        # print(f\"After Flattening shape is {x.shape}\")\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x) \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining Hyperparameters\n",
    "batch_size = 4\n",
    "\n",
    "# Discount factor for Belmann equation\n",
    "gamma = 0.99\n",
    "\n",
    "# Start value of the exploration rate\n",
    "eps_start = 1\n",
    "\n",
    "# End value of the exploration rate\n",
    "eps_end = 0.01\n",
    "\n",
    "# Decay rate to decay decay epsilon over time\n",
    "eps_decay = 0.001\n",
    "\n",
    "# How frequently the weights of target Network will be updated, in terms of episodes\n",
    "target_update = 10\n",
    "\n",
    "# Capacity of replay memory\n",
    "memory_size = 100_000\n",
    "\n",
    "# Learning rate of policy network\n",
    "lr = 0.001\n",
    "\n",
    "# Total number of episodes, dumbo\n",
    "num_episodes = 12\n",
    "\n",
    "# Setting up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Setting loss\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Hello\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'environ.reset()\\nenviron.start()\\nstate = get_ss.screenshot()\\nenviron.close_all()'"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "# Environment goes brr\n",
    "\n",
    "environ = ChromeManager(device=device)\n",
    "\n",
    "# Strategy goes brr\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "# Agent goes brr\n",
    "agent = Agent(strategy, 3, device)\n",
    "\n",
    "# Replay memory goes brr\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "# Defining our Policy network and target network\n",
    "policy_net = DinoNetwork().to(device)\n",
    "target_net = DinoNetwork().to(device)\n",
    "\n",
    "# For taking screenshots\n",
    "get_ss = ImagePreProcessing(environ.driver, device)\n",
    "\n",
    "# Now we set the weights of our target net to be the same as our policy net\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Choosing our optimizer\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "\n",
    "\"\"\"environ.reset()\n",
    "environ.start()\n",
    "state = get_ss.screenshot()\n",
    "environ.close_all()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Converting batches of exp to exp of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.next_state)\n",
    "    t4 = torch.cat(batch.reward)\n",
    "    t5 = torch.cat(batch.done)\n",
    "\n",
    "    return (t1, t2, t3, t4, t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train():    \n",
    "    possible_actions = ['Jump', 'Duck', 'Do Nothing']\n",
    "    for episodes in range(num_episodes):\n",
    "        environ.reset()\n",
    "        environ.start()\n",
    "        state = get_ss.screenshot()\n",
    "        done = False\n",
    "        # While the episode is running\n",
    "        # print(type(done))\n",
    "        while not done:\n",
    "            # print(type(done))\n",
    "            action = agent.select_action(state, policy_net)\n",
    "            print(f\"The agent decided to {possible_actions[action]}\")\n",
    "            next_state, reward, done, score, highscore  = environ.take_action(action)\n",
    "            \n",
    "            # Converting action, reward, done to Pytorch tensors\n",
    "            action_tensor = torch.tensor(action, dtype=torch.int64, device=device)\n",
    "            reward_tensor = torch.tensor(reward, dtype=torch.int64, device=device)\n",
    "            done_tensor = torch.tensor(done, dtype=torch.bool, device=device)\n",
    "            #print(f\"After changing, action :{type(action_tensor)}\\ndone: {type(done_tensor)}\\treward: {type(reward_tensor)}\")\n",
    "            memory.push(Experience(state, action_tensor.unsqueeze(0), next_state, reward_tensor.unsqueeze(0), done_tensor.unsqueeze(0)))\n",
    "            \n",
    "\n",
    "            # print(f\"action: {action_tensor}\\treward: {reward_tensor}\\ndone: {done_tensor}\")\n",
    "            \n",
    "            \"\"\"if memory.can_provide_sample(batch_size):\n",
    "                experiences = memory.sample(batch_size)\n",
    "                states, actions, rewards, next_states = extract_tensors(experiences)\"\"\"\n",
    "            # print(f\"batch size is {batch_size} an memory size is {memory_size}, and currently we have {memory.len()} samples. So we can provide samples? {memory.can_provide_sample(batch_size)}\")\n",
    "            if not memory.can_provide_sample(batch_size):\n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "                experiences_minibatch = memory.sample(batch_size)\n",
    "                # print(f\"Shape of exp minibatch{len(experiences_minibatch)}\")\n",
    "                experience_tensors = extract_tensors(experiences_minibatch)\n",
    "                states, actions, next_states, rewards, terminal = experience_tensors\n",
    "\n",
    "                # Get the current state from the policy network, pass it to the target n/w to get the q values of the next state.\n",
    "                # Different logic for terminating state and a normal state\n",
    "                current_q_values =  policy_net.forward(states)\n",
    "                next_q_values = target_net.forward(next_states)\n",
    "                \n",
    "                #print(experiences_minibatch)\n",
    "                \"\"\"q_values = torch.cat(tuple(rewards[i] if experiences_minibatch[i][terminal]\n",
    "                                    else reward[i] + gamma*torch.max(next_q_values[i])\n",
    "                                    for i in range(len(experiences_minibatch))))\"\"\"\n",
    "                # print(f\"rewards: {rewards}\\tactions: {actions}\\nterminal: {terminal}\")\n",
    "                # print(f\"Rewards = {actions} \\t rewards[1] = {rewards[1]}\")\n",
    "                # print(f\"Next_q_values: {next_q_values}\\tNext Q values[1]{next_q_values[0]}\")\n",
    "                # print(f\"Experiences minibatch {experiences_minibatch[1][4]}\")\n",
    "                q_values = torch.stack(tuple(rewards[i] if experiences_minibatch[i][4] \n",
    "                                    else rewards[i] + gamma*torch.max(next_q_values[i]) \n",
    "                                    for i in range(len(experiences_minibatch))))\n",
    "               \n",
    "                \n",
    "                y_pred = current_q_values.gather(1, actions.unsqueeze(1))\n",
    "                # print(f\"Y_pred: {y_pred}\")\n",
    "                optimizer.zero_grad()\n",
    "                q_values.detach()\n",
    "                loss = criterion(y_pred, q_values)\n",
    "                state = next_state\n",
    "\n",
    "        if num_episodes % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "THis marks the end of th capturing process\nThe agent decided to Jump\nTHis marks the end of th capturing process\nThe agent decided to Do Nothing\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Duck\nTHis marks the end of th capturing process\nThe agent decided to Duck\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Do Nothing\nTHis marks the end of th capturing process\nThe agent decided to Jump\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Duck\nTHis marks the end of th capturing process\nThe agent decided to Jump\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Duck\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Do Nothing\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Jump\nTHis marks the end of th capturing process\nThe agent decided to Jump\nTHis marks the end of th capturing process\nThe agent decided to Jump\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Do Nothing\nTHis marks the end of th capturing process\nThe agent decided to Do Nothing\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Jump\nTHis marks the end of th capturing process\nThe agent decided to Duck\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Duck\nTHis marks the end of th capturing process\nThe agent decided to Do Nothing\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Do Nothing\nTHis marks the end of th capturing process\nThe agent decided to Jump\nTHis marks the end of th capturing process\nThe agent decided to Jump\nTHis marks the end of th capturing process\nTHis marks the end of th capturing process\nThe agent decided to Duck\nTHis marks the end of th capturing process\nThe agent decided to Do Nothing\nTHis marks the end of th capturing process\n"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "10%3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python37864bit493ebcdfe64d44eab888e13e0887aab6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}